{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7766792b",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546e09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics„ÅÆYOLOv8„Çí‰ΩøÁî®„Åó„Å¶ÁîªÂÉèÊé®Ë´ñ„ÇíÂÆüË°å\n",
    "from ultralytics import YOLO\n",
    "import os \n",
    "\n",
    "model = YOLO(r\"D:\\prog_py\\Yolo_trainer\\runs\\train\\train15\\weights\\best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊåáÂÆö„Åï„Çå„Åü„Éë„É©„É°„Éº„Çø„ÅßÁîªÂÉèÊé®Ë´ñ„ÇíÂÆüË°å\n",
    "print(\"Êé®Ë´ñ„ÇíÈñãÂßã„Åó„Åæ„Åô...\")\n",
    "\n",
    "source_path = r\"D:\\prog_py\\yolo12_detect\\detect_images\"\n",
    "if os.path.exists(source_path):\n",
    "    print(\"üîç ÁîªÂÉè„Çµ„Ç§„Ç∫Ôºà32„ÅÆÂÄçÊï∞, 1500„Åæ„ÅßÔºâ„ÅßÊé®Ë´ñ„ÇíÂÆüË°å„Åó„Å¶„ÅÑ„Åæ„Åô...\")\n",
    "\n",
    "    # 32„ÅÆÂÄçÊï∞„Åß512„Åã„Çâ1248„Åæ„Åß\n",
    "    grid_imgsz = list(range(512, 1248, 32))\n",
    "\n",
    "    for imgsz in grid_imgsz:\n",
    "        print(f\"\\n--- ÁîªÂÉè„Çµ„Ç§„Ç∫: {imgsz} „ÅßÊé®Ë´ñ‰∏≠ ---\")\n",
    "        # ÂÆüÈ®ìÂêç„ÇíÁîªÂÉè„Çµ„Ç§„Ç∫„Åî„Å®„Å´ÂàÜ„Åë„Å¶‰øùÂ≠ò„Åô„Çã„Åì„Å®„ÅßÂêÑ„Çµ„Ç§„Ç∫„Åî„Å®„Å´ÁµêÊûú„ÅåË¶ã„ÇÑ„Åô„Åè„ÄÅ„Åô„Åπ„Å¶„ÅÆÁîªÂÉè„Çµ„Ç§„Ç∫„ÅßÊé®Ë´ñ„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åô„Çã\n",
    "        results = model.predict(\n",
    "            source=source_path,           # ÁîªÂÉè„Éï„Ç©„É´„ÉÄ: detect_images\n",
    "            imgsz=imgsz,                  # ÁîªÂÉè„Çµ„Ç§„Ç∫„ÇíÂãïÁöÑ„Å´Â§âÊõ¥\n",
    "            save_txt=True,                # „ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´‰øùÂ≠ò: True\n",
    "            line_width=1,                 # „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅÆÁ∑ö„ÅÆÂ§™„Åï: 1\n",
    "            save=True,                    # Êé®Ë´ñÁµêÊûú„ÅÆÁîªÂÉè„Çí‰øùÂ≠ò\n",
    "            conf=0.25,                    # ‰ø°È†ºÂ∫¶„ÅÆ„Åó„Åç„ÅÑÂÄ§\n",
    "            device='0',                   # „Éá„Éê„Ç§„ÇπÔºàGPU„ÅåÂà©Áî®ÂèØËÉΩ„Å™Â†¥Âêà„ÅØ'0'„Å´Â§âÊõ¥Ôºâ\n",
    "            project='runs/detect',        # ÁµêÊûú‰øùÂ≠ò„Éá„Ç£„É¨„ÇØ„Éà„É™\n",
    "            name=f'yolo_detection_{imgsz}',# ÁîªÂÉè„Çµ„Ç§„Ç∫„Åî„Å®„Å´„Éá„Ç£„É¨„ÇØ„Éà„É™ÂàÜ„Åë\n",
    "            agnostic_nms=True,\n",
    "        )\n",
    "\n",
    "print(\"‚úì Êé®Ë´ñ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„ÅüÔºÅ\")\n",
    "print(f\"ÁµêÊûú„ÅØ runs/detect/yolo_detection_ÔºúimgszÔºû „Éá„Ç£„É¨„ÇØ„Éà„É™„Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5598a",
   "metadata": {},
   "source": [
    "## predict single imgsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369417cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING imgsz=[1500] must be multiple of max stride 32, updating to [1504]\n",
      "image 1/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_000003.jpg: 1504x1376 74 items, 272.9ms\n",
      "image 2/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_000118.jpg: 1504x896 72 items, 107.5ms\n",
      "image 3/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_000188.jpg: 1504x864 64 items, 98.7ms\n",
      "image 4/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_000473.jpg: 1504x832 64 items, 111.9ms\n",
      "image 5/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_000565.jpg: 1504x608 102 items, 110.0ms\n",
      "image 6/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_000896.jpg: 1504x672 63 items, 100.1ms\n",
      "image 7/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_001075.jpg: 1504x1088 62 items, 95.4ms\n",
      "image 8/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_001845.jpg: 1504x576 62 items, 113.8ms\n",
      "image 9/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_001851.jpg: 1504x736 34 items, 96.4ms\n",
      "image 10/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_001885.jpg: 1504x512 44 items, 99.6ms\n",
      "image 11/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_001997.jpg: 1504x768 91 items, 97.1ms\n",
      "image 12/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_002005.jpg: 1504x736 66 items, 26.0ms\n",
      "image 13/13 D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\\Image_002036.jpg: 1504x544 67 items, 98.3ms\n",
      "Speed: 16.6ms preprocess, 109.8ms inference, 102.7ms postprocess per image at shape (1, 3, 1504, 544)\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\segment\\predict\u001b[0m\n",
      "13 labels saved to D:\\prog_py\\yolo12_detect\\runs\\segment\\predict\\labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[ 85, 115, 120],\n",
       "         [ 89, 119, 124],\n",
       "         [ 97, 126, 130],\n",
       "         ...,\n",
       "         [118, 156, 161],\n",
       "         [101, 136, 140],\n",
       "         [ 98, 130, 135]],\n",
       " \n",
       "        [[ 84, 113, 120],\n",
       "         [ 92, 121, 128],\n",
       "         [103, 133, 138],\n",
       "         ...,\n",
       "         [126, 162, 168],\n",
       "         [112, 147, 151],\n",
       "         [ 99, 131, 136]],\n",
       " \n",
       "        [[ 97, 128, 137],\n",
       "         [105, 136, 145],\n",
       "         [118, 149, 158],\n",
       "         ...,\n",
       "         [130, 164, 170],\n",
       "         [126, 158, 163],\n",
       "         [ 99, 129, 134]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 92, 130, 162],\n",
       "         [108, 146, 178],\n",
       "         [125, 163, 195],\n",
       "         ...,\n",
       "         [190, 198, 205],\n",
       "         [179, 189, 196],\n",
       "         [172, 182, 189]],\n",
       " \n",
       "        [[127, 165, 197],\n",
       "         [128, 166, 198],\n",
       "         [135, 173, 205],\n",
       "         ...,\n",
       "         [171, 182, 190],\n",
       "         [186, 196, 206],\n",
       "         [185, 198, 206]],\n",
       " \n",
       "        [[120, 158, 190],\n",
       "         [127, 165, 197],\n",
       "         [123, 161, 193],\n",
       "         ...,\n",
       "         [191, 203, 213],\n",
       "         [223, 235, 247],\n",
       "         [214, 229, 238]]], dtype=uint8)\n",
       " orig_shape: (1065, 958)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_000003.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 87.62850001221523, 'inference': 272.8888000128791, 'postprocess': 119.41739998292178},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[244, 241, 236],\n",
       "         [244, 241, 236],\n",
       "         [244, 241, 236],\n",
       "         ...,\n",
       "         [250, 251, 247],\n",
       "         [245, 249, 244],\n",
       "         [244, 248, 243]],\n",
       " \n",
       "        [[245, 242, 237],\n",
       "         [245, 242, 237],\n",
       "         [245, 242, 237],\n",
       "         ...,\n",
       "         [250, 254, 249],\n",
       "         [248, 252, 247],\n",
       "         [250, 254, 249]],\n",
       " \n",
       "        [[245, 242, 237],\n",
       "         [245, 242, 237],\n",
       "         [245, 242, 237],\n",
       "         ...,\n",
       "         [244, 248, 243],\n",
       "         [241, 245, 240],\n",
       "         [244, 248, 243]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 49, 149, 167],\n",
       "         [ 71, 171, 189],\n",
       "         [ 81, 181, 199],\n",
       "         ...,\n",
       "         [ 21,  43,  68],\n",
       "         [ 54,  76, 101],\n",
       "         [ 94, 118, 142]],\n",
       " \n",
       "        [[ 51, 151, 169],\n",
       "         [ 73, 173, 191],\n",
       "         [ 82, 182, 200],\n",
       "         ...,\n",
       "         [ 45,  67,  92],\n",
       "         [ 70,  94, 118],\n",
       "         [ 97, 121, 145]],\n",
       " \n",
       "        [[ 56, 156, 174],\n",
       "         [ 75, 175, 193],\n",
       "         [ 81, 181, 199],\n",
       "         ...,\n",
       "         [ 86, 109, 135],\n",
       "         [110, 133, 159],\n",
       "         [145, 168, 194]]], dtype=uint8)\n",
       " orig_shape: (1397, 821)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_000118.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 12.352399993687868, 'inference': 107.47859999537468, 'postprocess': 106.7992000025697},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 34,  32,  31],\n",
       "         [ 38,  36,  35],\n",
       "         [ 32,  30,  29],\n",
       "         ...,\n",
       "         [113, 103,  96],\n",
       "         [112, 102,  95],\n",
       "         [113, 101,  95]],\n",
       " \n",
       "        [[ 42,  40,  39],\n",
       "         [ 38,  36,  35],\n",
       "         [ 35,  33,  32],\n",
       "         ...,\n",
       "         [106,  94,  90],\n",
       "         [113, 101,  97],\n",
       "         [109,  99,  92]],\n",
       " \n",
       "        [[ 29,  27,  26],\n",
       "         [ 26,  24,  23],\n",
       "         [ 26,  24,  23],\n",
       "         ...,\n",
       "         [107,  95,  91],\n",
       "         [110,  98,  94],\n",
       "         [109,  99,  92]]], dtype=uint8)\n",
       " orig_shape: (1362, 769)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_000188.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 13.431100000161678, 'inference': 98.71560003375635, 'postprocess': 92.75899996282533},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[250, 255, 255],\n",
       "         [249, 255, 254],\n",
       "         [250, 255, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[250, 255, 254],\n",
       "         [249, 254, 253],\n",
       "         [253, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[252, 254, 254],\n",
       "         [250, 252, 252],\n",
       "         [255, 255, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 62,  74,  98],\n",
       "         [116, 128, 152],\n",
       "         [183, 195, 219],\n",
       "         ...,\n",
       "         [125, 181, 186],\n",
       "         [127, 179, 186],\n",
       "         [125, 177, 184]],\n",
       " \n",
       "        [[118, 131, 153],\n",
       "         [172, 185, 207],\n",
       "         [190, 203, 225],\n",
       "         ...,\n",
       "         [121, 176, 185],\n",
       "         [120, 170, 182],\n",
       "         [117, 165, 177]],\n",
       " \n",
       "        [[ 89, 102, 124],\n",
       "         [150, 163, 185],\n",
       "         [148, 161, 183],\n",
       "         ...,\n",
       "         [118, 170, 182],\n",
       "         [111, 160, 174],\n",
       "         [108, 155, 169]]], dtype=uint8)\n",
       " orig_shape: (1142, 614)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_000473.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 9.105800010729581, 'inference': 111.88749998109415, 'postprocess': 109.95730001013726},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[252, 251, 237],\n",
       "         [252, 251, 237],\n",
       "         [252, 251, 237],\n",
       "         ...,\n",
       "         [ 40,  84,  77],\n",
       "         [ 39,  83,  76],\n",
       "         [ 30,  77,  69]],\n",
       " \n",
       "        [[252, 251, 237],\n",
       "         [252, 251, 237],\n",
       "         [252, 251, 237],\n",
       "         ...,\n",
       "         [ 38,  80,  73],\n",
       "         [ 39,  83,  76],\n",
       "         [ 34,  81,  73]],\n",
       " \n",
       "        [[252, 251, 237],\n",
       "         [252, 251, 237],\n",
       "         [252, 251, 237],\n",
       "         ...,\n",
       "         [ 36,  79,  70],\n",
       "         [ 39,  83,  76],\n",
       "         [ 36,  83,  75]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[176, 220, 214],\n",
       "         [182, 226, 219],\n",
       "         [173, 219, 213],\n",
       "         ...,\n",
       "         [ 96, 105, 119],\n",
       "         [124, 133, 147],\n",
       "         [123, 132, 146]],\n",
       " \n",
       "        [[182, 225, 222],\n",
       "         [177, 221, 215],\n",
       "         [185, 228, 225],\n",
       "         ...,\n",
       "         [ 87,  96, 110],\n",
       "         [123, 132, 146],\n",
       "         [122, 131, 145]],\n",
       " \n",
       "        [[183, 226, 223],\n",
       "         [182, 225, 222],\n",
       "         [202, 245, 242],\n",
       "         ...,\n",
       "         [ 46,  55,  69],\n",
       "         [ 84,  93, 107],\n",
       "         [ 98, 107, 121]]], dtype=uint8)\n",
       " orig_shape: (1746, 704)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_000565.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 12.523500015959144, 'inference': 109.95519999414682, 'postprocess': 137.67929997993633},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[240, 226, 214],\n",
       "         [240, 226, 214],\n",
       "         [240, 226, 214],\n",
       "         ...,\n",
       "         [226, 212, 200],\n",
       "         [226, 212, 200],\n",
       "         [226, 212, 200]],\n",
       " \n",
       "        [[240, 226, 214],\n",
       "         [240, 226, 214],\n",
       "         [240, 226, 214],\n",
       "         ...,\n",
       "         [225, 211, 199],\n",
       "         [225, 211, 199],\n",
       "         [225, 211, 199]],\n",
       " \n",
       "        [[240, 226, 214],\n",
       "         [240, 226, 214],\n",
       "         [239, 225, 213],\n",
       "         ...,\n",
       "         [225, 211, 199],\n",
       "         [225, 211, 199],\n",
       "         [225, 211, 199]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 80, 114, 137],\n",
       "         [ 92, 126, 149],\n",
       "         [ 97, 131, 154],\n",
       "         ...,\n",
       "         [  0,   7,  21],\n",
       "         [  0,   0,  15],\n",
       "         [  0,   6,  23]],\n",
       " \n",
       "        [[ 88, 122, 145],\n",
       "         [117, 151, 174],\n",
       "         [122, 156, 179],\n",
       "         ...,\n",
       "         [ 59,  68,  82],\n",
       "         [ 10,  18,  35],\n",
       "         [  0,   3,  20]],\n",
       " \n",
       "        [[ 90, 125, 145],\n",
       "         [116, 151, 171],\n",
       "         [124, 159, 179],\n",
       "         ...,\n",
       "         [ 79,  89, 106],\n",
       "         [ 49,  60,  74],\n",
       "         [ 37,  50,  64]]], dtype=uint8)\n",
       " orig_shape: (1585, 680)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_000896.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 11.321400001179427, 'inference': 100.1045000157319, 'postprocess': 104.63770001661032},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 94, 110, 127],\n",
       "         [ 91, 107, 124],\n",
       "         [ 90, 104, 122],\n",
       "         ...,\n",
       "         [ 32,  40,  47],\n",
       "         [ 22,  30,  37],\n",
       "         [ 28,  36,  43]],\n",
       " \n",
       "        [[ 98, 114, 131],\n",
       "         [ 92, 108, 125],\n",
       "         [ 94, 108, 126],\n",
       "         ...,\n",
       "         [ 33,  41,  48],\n",
       "         [ 17,  25,  32],\n",
       "         [ 23,  31,  38]],\n",
       " \n",
       "        [[ 85, 100, 116],\n",
       "         [ 81,  96, 112],\n",
       "         [ 73,  88, 104],\n",
       "         ...,\n",
       "         [ 30,  36,  43],\n",
       "         [ 19,  25,  32],\n",
       "         [ 21,  27,  34]]], dtype=uint8)\n",
       " orig_shape: (1417, 1015)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_001075.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 16.352800012100488, 'inference': 95.39249999215826, 'postprocess': 85.90750000439584},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[233, 222, 218],\n",
       "         [233, 222, 218],\n",
       "         [233, 222, 218],\n",
       "         ...,\n",
       "         [ 83, 141, 153],\n",
       "         [126, 176, 188],\n",
       "         [157, 199, 211]],\n",
       " \n",
       "        [[235, 224, 220],\n",
       "         [235, 224, 220],\n",
       "         [235, 224, 220],\n",
       "         ...,\n",
       "         [ 80, 140, 152],\n",
       "         [114, 166, 178],\n",
       "         [144, 189, 200]],\n",
       " \n",
       "        [[236, 225, 221],\n",
       "         [236, 225, 221],\n",
       "         [236, 225, 221],\n",
       "         ...,\n",
       "         [ 75, 137, 148],\n",
       "         [ 95, 149, 160],\n",
       "         [140, 189, 199]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  2,   2,  26],\n",
       "         [  3,   6,  27],\n",
       "         [  0,   7,  27],\n",
       "         ...,\n",
       "         [ 62,  98, 136],\n",
       "         [ 66, 103, 141],\n",
       "         [ 66, 102, 142]],\n",
       " \n",
       "        [[  2,   4,  28],\n",
       "         [  0,   4,  28],\n",
       "         [  0,   6,  26],\n",
       "         ...,\n",
       "         [ 45,  81, 121],\n",
       "         [ 55,  91, 131],\n",
       "         [ 56,  94, 136]],\n",
       " \n",
       "        [[  5,   7,  31],\n",
       "         [  0,   4,  28],\n",
       "         [  0,   7,  27],\n",
       "         ...,\n",
       "         [ 68, 104, 144],\n",
       "         [ 60,  96, 136],\n",
       "         [ 60,  98, 140]]], dtype=uint8)\n",
       " orig_shape: (1571, 601)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_001845.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 10.204999998677522, 'inference': 113.84810000890866, 'postprocess': 86.12759999232367},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[ 83, 168, 164],\n",
       "         [ 85, 170, 166],\n",
       "         [ 97, 178, 175],\n",
       "         ...,\n",
       "         [ 88,  96,  96],\n",
       "         [ 95, 100,  99],\n",
       "         [115, 120, 119]],\n",
       " \n",
       "        [[ 81, 166, 162],\n",
       "         [ 83, 167, 163],\n",
       "         [ 99, 180, 177],\n",
       "         ...,\n",
       "         [ 76,  84,  84],\n",
       "         [ 91,  96,  95],\n",
       "         [120, 125, 124]],\n",
       " \n",
       "        [[ 81, 165, 161],\n",
       "         [ 78, 162, 158],\n",
       "         [ 96, 177, 174],\n",
       "         ...,\n",
       "         [115, 120, 121],\n",
       "         [133, 135, 135],\n",
       "         [161, 163, 163]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[127, 123, 118],\n",
       "         [172, 168, 163],\n",
       "         [209, 205, 200],\n",
       "         ...,\n",
       "         [ 16,  37,  65],\n",
       "         [ 42,  63,  91],\n",
       "         [ 51,  72, 100]],\n",
       " \n",
       "        [[163, 159, 154],\n",
       "         [183, 179, 174],\n",
       "         [199, 195, 190],\n",
       "         ...,\n",
       "         [ 12,  33,  61],\n",
       "         [ 45,  66,  94],\n",
       "         [ 60,  81, 109]],\n",
       " \n",
       "        [[134, 130, 125],\n",
       "         [123, 119, 114],\n",
       "         [126, 122, 117],\n",
       "         ...,\n",
       "         [ 10,  31,  59],\n",
       "         [ 47,  68,  96],\n",
       "         [ 64,  85, 113]]], dtype=uint8)\n",
       " orig_shape: (1492, 723)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_001851.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 10.302199982106686, 'inference': 96.40420001232997, 'postprocess': 74.22690000385046},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[183, 195, 199],\n",
       "         [212, 224, 228],\n",
       "         [227, 239, 245],\n",
       "         ...,\n",
       "         [250, 252, 252],\n",
       "         [250, 252, 252],\n",
       "         [251, 253, 253]],\n",
       " \n",
       "        [[130, 145, 148],\n",
       "         [168, 183, 186],\n",
       "         [199, 213, 219],\n",
       "         ...,\n",
       "         [251, 253, 253],\n",
       "         [252, 254, 254],\n",
       "         [252, 254, 254]],\n",
       " \n",
       "        [[115, 135, 140],\n",
       "         [125, 145, 150],\n",
       "         [145, 163, 170],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[139, 156, 177],\n",
       "         [130, 149, 170],\n",
       "         [129, 147, 170],\n",
       "         ...,\n",
       "         [ 28,  36,  53],\n",
       "         [ 34,  42,  59],\n",
       "         [ 34,  44,  61]],\n",
       " \n",
       "        [[144, 161, 182],\n",
       "         [139, 156, 177],\n",
       "         [128, 144, 167],\n",
       "         ...,\n",
       "         [  0,   5,  22],\n",
       "         [  0,   7,  24],\n",
       "         [  0,  10,  27]],\n",
       " \n",
       "        [[ 91, 106, 125],\n",
       "         [ 88, 105, 124],\n",
       "         [ 79,  96, 117],\n",
       "         ...,\n",
       "         [  6,  14,  31],\n",
       "         [  4,  13,  27],\n",
       "         [  3,  14,  28]]], dtype=uint8)\n",
       " orig_shape: (1463, 490)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_001885.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 6.0116000240668654, 'inference': 99.64650002075359, 'postprocess': 89.70450004562736},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[243, 243, 243],\n",
       "         [243, 243, 243],\n",
       "         [243, 243, 243],\n",
       "         ...,\n",
       "         [244, 244, 244],\n",
       "         [244, 244, 244],\n",
       "         [244, 244, 244]],\n",
       " \n",
       "        [[243, 243, 243],\n",
       "         [243, 243, 243],\n",
       "         [243, 243, 243],\n",
       "         ...,\n",
       "         [244, 244, 244],\n",
       "         [244, 244, 244],\n",
       "         [244, 244, 244]],\n",
       " \n",
       "        [[243, 243, 243],\n",
       "         [243, 243, 243],\n",
       "         [243, 243, 243],\n",
       "         ...,\n",
       "         [244, 244, 244],\n",
       "         [244, 244, 244],\n",
       "         [244, 244, 244]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[242, 246, 241],\n",
       "         [243, 247, 242],\n",
       "         [241, 244, 242],\n",
       "         ...,\n",
       "         [ 51,  89, 124],\n",
       "         [ 45,  83, 118],\n",
       "         [ 53,  93, 128]],\n",
       " \n",
       "        [[241, 245, 240],\n",
       "         [243, 247, 242],\n",
       "         [241, 244, 242],\n",
       "         ...,\n",
       "         [ 42,  80, 115],\n",
       "         [ 53,  91, 126],\n",
       "         [ 62, 102, 137]],\n",
       " \n",
       "        [[243, 244, 242],\n",
       "         [245, 246, 244],\n",
       "         [242, 245, 243],\n",
       "         ...,\n",
       "         [ 25,  66,  99],\n",
       "         [ 33,  74, 107],\n",
       "         [ 47,  88, 121]]], dtype=uint8)\n",
       " orig_shape: (1573, 801)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_001997.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 10.4614999727346, 'inference': 97.12319995742291, 'postprocess': 133.03750002523884},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[ 14,  26,  50],\n",
       "         [  8,  21,  43],\n",
       "         [ 17,  28,  50],\n",
       "         ...,\n",
       "         [ 58, 172, 212],\n",
       "         [ 52, 165, 205],\n",
       "         [ 50, 161, 201]],\n",
       " \n",
       "        [[ 16,  28,  52],\n",
       "         [ 12,  25,  47],\n",
       "         [ 20,  31,  51],\n",
       "         ...,\n",
       "         [ 49, 164, 201],\n",
       "         [ 47, 158, 198],\n",
       "         [ 51, 162, 200]],\n",
       " \n",
       "        [[ 13,  26,  48],\n",
       "         [  5,  18,  40],\n",
       "         [ 11,  22,  42],\n",
       "         ...,\n",
       "         [ 51, 167, 202],\n",
       "         [ 51, 162, 200],\n",
       "         [ 58, 168, 204]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[109, 140, 141],\n",
       "         [110, 141, 142],\n",
       "         [110, 141, 142],\n",
       "         ...,\n",
       "         [ 99, 159, 151],\n",
       "         [ 79, 146, 139],\n",
       "         [ 74, 142, 135]],\n",
       " \n",
       "        [[123, 155, 154],\n",
       "         [131, 163, 162],\n",
       "         [141, 172, 173],\n",
       "         ...,\n",
       "         [129, 187, 176],\n",
       "         [ 97, 160, 150],\n",
       "         [ 81, 147, 136]],\n",
       " \n",
       "        [[133, 165, 164],\n",
       "         [142, 174, 173],\n",
       "         [151, 182, 183],\n",
       "         ...,\n",
       "         [145, 201, 188],\n",
       "         [119, 181, 169],\n",
       "         [101, 165, 153]]], dtype=uint8)\n",
       " orig_shape: (1152, 550)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_002005.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 9.312300011515617, 'inference': 26.01559995673597, 'postprocess': 94.74580001551658},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'item'}\n",
       " obb: None\n",
       " orig_img: array([[[132, 163, 186],\n",
       "         [139, 170, 193],\n",
       "         [150, 176, 200],\n",
       "         ...,\n",
       "         [122, 152, 139],\n",
       "         [123, 153, 140],\n",
       "         [122, 152, 139]],\n",
       " \n",
       "        [[149, 180, 201],\n",
       "         [150, 181, 202],\n",
       "         [153, 182, 203],\n",
       "         ...,\n",
       "         [121, 151, 138],\n",
       "         [121, 151, 138],\n",
       "         [119, 149, 136]],\n",
       " \n",
       "        [[138, 171, 190],\n",
       "         [137, 170, 189],\n",
       "         [139, 171, 190],\n",
       "         ...,\n",
       "         [120, 150, 139],\n",
       "         [120, 150, 139],\n",
       "         [117, 150, 136]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 90, 118, 153],\n",
       "         [115, 143, 178],\n",
       "         [121, 149, 184],\n",
       "         ...,\n",
       "         [ 65, 122, 131],\n",
       "         [ 97, 159, 165],\n",
       "         [117, 181, 186]],\n",
       " \n",
       "        [[ 73, 101, 136],\n",
       "         [105, 133, 168],\n",
       "         [125, 153, 188],\n",
       "         ...,\n",
       "         [104, 163, 173],\n",
       "         [ 96, 158, 166],\n",
       "         [ 97, 160, 168]],\n",
       " \n",
       "        [[ 64,  92, 127],\n",
       "         [ 90, 118, 153],\n",
       "         [121, 149, 184],\n",
       "         ...,\n",
       "         [147, 205, 217],\n",
       "         [141, 202, 212],\n",
       "         [137, 200, 208]]], dtype=uint8)\n",
       " orig_shape: (1518, 521)\n",
       " path: 'D:\\\\prog_py\\\\yolo12_detect\\\\detect_p2pnet_images\\\\Image_002036.jpg'\n",
       " probs: None\n",
       " save_dir: 'D:\\\\prog_py\\\\yolo12_detect\\\\runs\\\\segment\\\\predict'\n",
       " speed: {'preprocess': 6.650699942838401, 'inference': 98.28869998455048, 'postprocess': 99.93099997518584}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(source= r\"D:\\prog_py\\yolo12_detect\\detect_p2pnet_images\", \n",
    "            save=True,\n",
    "            imgsz=1500, \n",
    "            conf=0.5,\n",
    "            save_txt=True,                # „ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´‰øùÂ≠ò: True\n",
    "            line_width=1,\n",
    "            agnostic_nms=True  \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322475d8",
   "metadata": {},
   "source": [
    "## VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55c86a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ÁîªÂÉè„Çµ„Ç§„Ç∫Ôºà32„ÅÆÂÄçÊï∞, 1500„Åæ„ÅßÔºâ„ÅßÊ§úË®º„ÇíÂÆüË°å„Åó„Å¶„ÅÑ„Åæ„Åô...\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 512 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "YOLOv12n-seg summary (fused): 172 layers, 2,779,387 parameters, 0 gradients, 9.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 975.8518.9 MB/s, size: 881.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:13<3:01\n",
      "                   all         11       1001      0.525      0.307      0.339      0.151      0.507      0.282      0.296       0.11\n",
      "Speed: 3.8ms preprocess, 34.4ms inference, 0.0ms loss, 13.2ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_512\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_512\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 544 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1650.5296.5 MB/s, size: 1057.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:12<2:58\n",
      "                   all         11       1001      0.533      0.354      0.379      0.166      0.494       0.31      0.327      0.122\n",
      "Speed: 2.6ms preprocess, 30.8ms inference, 0.0ms loss, 16.5ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_544\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_544\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 576 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 1530.9264.9 MB/s, size: 1018.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:13<2:58\n",
      "                   all         11       1001      0.547      0.375      0.404      0.184      0.558      0.353      0.376      0.141\n",
      "Speed: 6.6ms preprocess, 43.0ms inference, 0.0ms loss, 19.1ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_576\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_576\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 608 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1456.5407.0 MB/s, size: 981.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:14<3:00\n",
      "                   all         11       1001      0.528      0.437      0.445      0.211      0.508      0.394      0.391      0.161\n",
      "Speed: 2.9ms preprocess, 30.3ms inference, 0.0ms loss, 21.5ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_608\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_608\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 640 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1870.0299.6 MB/s, size: 903.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:13<2:59\n",
      "                   all         11       1001      0.604      0.409      0.461      0.221      0.611      0.391      0.432      0.172\n",
      "Speed: 4.9ms preprocess, 26.2ms inference, 0.0ms loss, 25.4ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_640\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_640\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 672 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1570.4382.6 MB/s, size: 1161.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.1Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:12<2:58\n",
      "                   all         11       1001      0.654       0.43        0.5      0.254       0.65      0.405      0.464      0.194\n",
      "Speed: 4.4ms preprocess, 26.9ms inference, 0.0ms loss, 23.5ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_672\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_672\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 704 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 1069.4229.3 MB/s, size: 881.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 10.8Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:15<3:06\n",
      "                   all         11       1001      0.657      0.466      0.536      0.278      0.591      0.454      0.494      0.217\n",
      "Speed: 4.8ms preprocess, 26.2ms inference, 0.0ms loss, 77.3ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_704\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_704\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 736 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 2001.2507.1 MB/s, size: 1031.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:18<3:09\n",
      "                   all         11       1001      0.691      0.507      0.567      0.307       0.66       0.48      0.531      0.238\n",
      "Speed: 5.3ms preprocess, 51.9ms inference, 0.0ms loss, 116.3ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_736\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_736\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 768 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1393.0383.3 MB/s, size: 899.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:273:46\n",
      "                   all         11       1001      0.748      0.495      0.615      0.347      0.693      0.488      0.568      0.267\n",
      "Speed: 3.0ms preprocess, 31.4ms inference, 0.0ms loss, 190.8ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_768\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_768\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 800 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1748.6366.3 MB/s, size: 912.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.2Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:253:40\n",
      "                   all         11       1001        0.7      0.571      0.645      0.369      0.742      0.512      0.612      0.293\n",
      "Speed: 7.8ms preprocess, 42.9ms inference, 0.0ms loss, 177.8ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_800\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_800\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 832 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 2037.2294.2 MB/s, size: 1153.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:323:59\n",
      "                   all         11       1001      0.737      0.599      0.676      0.414      0.726      0.584      0.648      0.327\n",
      "Speed: 6.3ms preprocess, 37.9ms inference, 0.0ms loss, 454.7ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_832\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_832\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 864 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 1148.7489.5 MB/s, size: 845.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:344:13\n",
      "                   all         11       1001      0.757      0.607        0.7      0.432      0.746        0.6      0.673      0.343\n",
      "Speed: 6.5ms preprocess, 30.2ms inference, 0.0ms loss, 537.6ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_864\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_864\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 896 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1519.5133.9 MB/s, size: 940.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:374:21\n",
      "                   all         11       1001      0.794      0.668       0.75      0.476      0.789      0.647       0.73      0.382\n",
      "Speed: 8.4ms preprocess, 45.6ms inference, 0.0ms loss, 557.5ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_896\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_896\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 928 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 2030.9225.8 MB/s, size: 1049.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:364:17\n",
      "                   all         11       1001      0.821      0.677       0.77      0.504      0.804      0.664       0.74      0.406\n",
      "Speed: 11.4ms preprocess, 46.7ms inference, 0.0ms loss, 464.1ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_928\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_928\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 960 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1605.1416.2 MB/s, size: 920.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 1:354:11\n",
      "                   all         11       1001       0.87        0.7      0.807      0.539      0.841      0.689      0.779      0.437\n",
      "Speed: 8.1ms preprocess, 37.5ms inference, 0.0ms loss, 540.1ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_960\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_960\u001b[0m\n",
      "\n",
      "--- ÁîªÂÉè„Çµ„Ç§„Ç∫: 992 „ÅßÊ§úË®º‰∏≠ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1262.0518.5 MB/s, size: 1083.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 11.1Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/2  35.8s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imgsz \u001b[38;5;129;01min\u001b[39;00m grid_imgsz:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- ÁîªÂÉè„Çµ„Ç§„Ç∫: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m „ÅßÊ§úË®º‰∏≠ ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m         validation_results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_yaml_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns/val\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_imgs_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimgsz\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå „Éá„Éº„Çø„Çª„ÉÉ„ÉàË®≠ÂÆö„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑ„Åü„ÇÅ„ÄÅÊ§úË®º„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô„ÄÇ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:635\u001b[0m, in \u001b[0;36mModel.val\u001b[1;34m(self, validator, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[0;32m    634\u001b[0m validator \u001b[38;5;241m=\u001b[39m (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidator\u001b[39m\u001b[38;5;124m\"\u001b[39m))(args\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 635\u001b[0m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m validator\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\validator.py:226\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[1;34m(self, trainer, model)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[38;5;241m3\u001b[39m]:\n\u001b[0;32m    224\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(preds)\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mplots \u001b[38;5;129;01mand\u001b[39;00m batch_i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_val_samples(batch, batch_i)\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\val.py:210\u001b[0m, in \u001b[0;36mDetectionValidator.update_metrics\u001b[1;34m(self, preds, batch)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_json \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n\u001b[1;32m--> 210\u001b[0m     predn_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_json:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_to_json(predn_scaled, pbatch)\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\models\\yolo\\segment\\val.py:245\u001b[0m, in \u001b[0;36mSegmentationValidator.scale_preds\u001b[1;34m(self, predn, pbatch)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscale_preds\u001b[39m(\u001b[38;5;28mself\u001b[39m, predn: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor], pbatch: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scales predictions to the original image size.\"\"\"\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mscale_preds(predn, pbatch),\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m: ops\u001b[38;5;241m.\u001b[39mscale_image(\n\u001b[1;32m--> 245\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmasks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m    246\u001b[0m             pbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mori_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    247\u001b[0m             ratio_pad\u001b[38;5;241m=\u001b[39mpbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratio_pad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    248\u001b[0m         ),\n\u001b[0;32m    249\u001b[0m     }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = YOLO(r\"D:\\prog_py\\Yolo_trainer\\runs\\train\\train15\\weights\\best.pt\")\n",
    "# dataset.yaml„ÅÆ„Éë„Çπ\n",
    "dataset_yaml_path = r\"D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\dataset.yaml\"\n",
    "\n",
    "# ÁîªÂÉè„Çµ„Ç§„Ç∫ÔºàimgszÔºâ„ÅØmax stride 32„ÅÆÂÄçÊï∞„Åß„Ç∞„É™„ÉÉ„Éâ„Çµ„Éº„ÉÅ\n",
    "if os.path.exists(dataset_yaml_path):\n",
    "    print(\"üîç ÁîªÂÉè„Çµ„Ç§„Ç∫Ôºà32„ÅÆÂÄçÊï∞, 1500„Åæ„ÅßÔºâ„ÅßÊ§úË®º„ÇíÂÆüË°å„Åó„Å¶„ÅÑ„Åæ„Åô...\")\n",
    "\n",
    "    # 32„ÅÆÂÄçÊï∞„Åß500„Åã„Çâ1504„Åæ„Åß\n",
    "    grid_imgsz = list(range(512, 1248, 32))\n",
    "\n",
    "    for imgsz in grid_imgsz:\n",
    "        print(f\"\\n--- ÁîªÂÉè„Çµ„Ç§„Ç∫: {imgsz} „ÅßÊ§úË®º‰∏≠ ---\")\n",
    "        validation_results = model.val(\n",
    "            data=dataset_yaml_path,\n",
    "            imgsz=imgsz,\n",
    "            plots=True,\n",
    "            batch=8,\n",
    "            save_json=True,\n",
    "            device='0',\n",
    "            project='runs/val',\n",
    "            name=f'val_imgs_{imgsz}',\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå „Éá„Éº„Çø„Çª„ÉÉ„ÉàË®≠ÂÆö„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑ„Åü„ÇÅ„ÄÅÊ§úË®º„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô„ÄÇ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3f4e0",
   "metadata": {},
   "source": [
    "## single val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a33570",
   "metadata": {},
   "source": [
    "##„ÄÄÂá∫ÂäõÂ§âÊèõ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb40a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING imgsz=[1400] must be multiple of max stride 32, updating to [1408]\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "YOLOv12n-seg summary (fused): 172 layers, 2,779,387 parameters, 0 gradients, 9.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 780.6332.2 MB/s, size: 1129.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\labels\\val.cache... 11 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\045_06_f.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\images\\val\\057_06_b.jpg: 2 duplicate labels removed\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.0it/s 2:487:58\n",
      "                   all         11       1001      0.971      0.928      0.968      0.835       0.97      0.927      0.966      0.692\n",
      "Speed: 49.2ms preprocess, 1933.0ms inference, 0.0ms loss, 2990.4ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1400\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1400\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(r\"D:\\prog_py\\Yolo_trainer\\runs\\train\\train15\\weights\\best.pt\")\n",
    "# dataset.yaml„ÅÆ„Éë„Çπ\n",
    "dataset_yaml_path = r\"D:\\prog_py\\Yolo_trainer\\ultralytic_series\\data\\all_datasets\\YOLODataset\\dataset.yaml\"\n",
    "imgsz = 1400\n",
    "\n",
    "validation_results = model.val(\n",
    "            data=dataset_yaml_path,\n",
    "            imgsz=imgsz,\n",
    "            plots=True,\n",
    "            batch=8,\n",
    "            save_json=True,\n",
    "            device='0',\n",
    "            project='runs/val',\n",
    "            name=f'val_imgs_{imgsz}',\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c58791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "\n",
    "model = YOLO(r\"D:\\prog_py\\Yolo_trainer\\runs\\train\\train9\\weights\\best.pt\")  # load a custom trained model\n",
    "\n",
    "# Export the model\n",
    "model.export(format=\"onnx\",\n",
    "             imgsz=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c0ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1204bd8",
   "metadata": {},
   "source": [
    "## auto label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d4539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING imgsz=[2000] must be multiple of max stride 32, updating to [2016]\n",
      "image 1/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\024_04_b.jpg: 2016x992 7 0spps, 2 1spps, 45 1spp_occs, 24 2spps, 42 2spp_occs, 2 3spps, 6 pod_occs, 132.7ms\n",
      "image 2/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\024_04_f.jpg: 2016x960 1 0spp, 3 1spps, 24 1spp_occs, 14 2spps, 32 2spp_occs, 3 3spp_occs, 2 pod_occs, 3518.6ms\n",
      "image 3/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\043_03_b.jpg: 2016x1216 2 0spps, 1 1spp, 16 1spp_occs, 14 2spps, 20 2spp_occs, 2 3spps, 2 3spp_occs, 6 pod_occs, 2549.8ms\n",
      "image 4/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\043_3_f.jpg: 2016x1056 3 0spps, 2 1spps, 18 1spp_occs, 10 2spps, 23 2spp_occs, 2 3spp_occs, 4 pod_occs, 2519.2ms\n",
      "image 5/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\045_05_b.jpg: 2016x1504 1 0spp, 1 1spp, 7 1spp_occs, 7 2spps, 10 2spp_occs, 1 pod_occ, 6221.5ms\n",
      "image 6/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\045_05_f.jpg: 2016x1536 2 0spps, 2 1spps, 12 1spp_occs, 9 2spps, 16 2spp_occs, 2 3spps, 5 3spp_occs, 8326.3ms\n",
      "image 7/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\052_03_b.jpg: 2016x992 3 0spps, 1 1spp, 46 1spp_occs, 4 2spps, 32 2spp_occs, 1 3spp_occ, 4 pod_occs, 1841.9ms\n",
      "image 8/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\052_03_f_un.jpg: 2016x1280 3 0spps, 3 1spps, 51 1spp_occs, 17 2spps, 35 2spp_occs, 1 3spp, 1 3spp_occ, 7 pod_occs, 4789.7ms\n",
      "image 9/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\057_02_b.jpg: 2016x1120 5 0spps, 1 1spp, 26 1spp_occs, 12 2spps, 30 2spp_occs, 1 3spp, 3 3spp_occs, 5 pod_occs, 3344.9ms\n",
      "image 10/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\057_02_fjpg.jpg: 2016x800 2 1spps, 43 1spp_occs, 7 2spps, 26 2spp_occs, 2 3spps, 1 3spp_occ, 9 pod_occs, 2244.7ms\n",
      "image 11/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\060_03_b.jpg: 2016x768 1 0spp, 2 1spps, 43 1spp_occs, 6 2spps, 33 2spp_occs, 1 3spp_occ, 3 pod_occs, 1344.3ms\n",
      "image 12/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\060_03_f.jpg: 2016x768 2 0spps, 2 1spps, 41 1spp_occs, 3 2spps, 21 2spp_occs, 1 3spp, 2 3spp_occs, 2 pod_occs, 1589.4ms\n",
      "image 13/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\111_03_b.jpg: 2016x1088 2 0spps, 4 1spps, 32 1spp_occs, 16 2spps, 34 2spp_occs, 1 3spp_occ, 11 pod_occs, 3399.7ms\n",
      "image 14/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\111_03_f.jpg: 2016x1312 2 0spps, 3 1spps, 45 1spp_occs, 26 2spps, 53 2spp_occs, 10 pod_occs, 3301.2ms\n",
      "image 15/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\134_05_f.jpg: 2016x2016 29 1spp_occs, 15 2spps, 36 2spp_occs, 1 3spp, 3 pod_occs, 11089.3ms\n",
      "image 16/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\135_03or04_b.jpg: 2016x1376 1 0spp, 2 1spps, 58 1spp_occs, 13 2spps, 37 2spp_occs, 5 3spps, 1 3spp_occ, 4 pod_occs, 3164.3ms\n",
      "image 17/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\135_03or04_f.jpg: 2016x1152 1 1spp, 44 1spp_occs, 21 2spps, 50 2spp_occs, 2 3spps, 1 3spp_occ, 4 pod_occs, 2109.8ms\n",
      "image 18/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\139_03_b.jpg: 2016x1216 1 0spp, 5 1spps, 48 1spp_occs, 11 2spps, 35 2spp_occs, 2 3spps, 2 3spp_occs, 4 pod_occs, 2556.3ms\n",
      "image 19/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\139_03_f.jpg: 2016x1376 8 0spps, 6 1spps, 72 1spp_occs, 20 2spps, 66 2spp_occs, 3 3spp_occs, 2 pod_occs, 3014.0ms\n",
      "image 20/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\144_03_b.jpg: 2016x1184 3 0spps, 1 1spp, 34 1spp_occs, 12 2spps, 25 2spp_occs, 1 3spp_occ, 1 pod_occ, 3165.5ms\n",
      "image 21/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\144_03_f.jpg: 2016x1152 6 0spps, 4 1spps, 47 1spp_occs, 9 2spps, 43 2spp_occs, 3 3spp_occs, 7 pod_occs, 1778.4ms\n",
      "image 22/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\162_03_b.jpg: 2016x1024 5 0spps, 11 1spps, 66 1spp_occs, 40 2spps, 49 2spp_occs, 1 3spp, 1 3spp_occ, 4 pod_occs, 2737.4ms\n",
      "image 23/23 E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\\162_03_f.jpg: 2016x1632 4 0spps, 9 1spps, 57 1spp_occs, 31 2spps, 50 2spp_occs, 3 3spps, 3 3spp_occs, 2 pod_occs, 4971.1ms\n",
      "Speed: 44.8ms preprocess, 3465.7ms inference, 1922.2ms postprocess per image at shape (1, 3, 2016, 1632)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.data.annotator import auto_annotate\n",
    "\n",
    "auto_annotate(data=r\"E:\\data\\pods\\pod2024\\Datase_2024_pod_insta360_backup\\test\\images\",\n",
    "              det_model=r\"D:\\prog_py\\Yolo_trainer\\runs\\train\\all_class_pod\\weights\\best.pt\",\n",
    "              imgsz=2000,\n",
    "              sam_model=\"sam2.1_l.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7766792b",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "546e09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralyticsã®YOLOv8ã‚’ä½¿ç”¨ã—ã¦ç”»åƒæŽ¨è«–ã‚’å®Ÿè¡Œ\n",
    "from ultralytics import YOLO\n",
    "import os \n",
    "\n",
    "model = YOLO(r\"D:\\prog_py\\Yolo_trainer\\runs\\train\\yolo12n-seg-pod\\weights\\best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c6340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŽ¨è«–ã‚’é–‹å§‹ã—ã¾ã™...\n",
      "ðŸ” ç”»åƒã‚µã‚¤ã‚ºï¼ˆ32ã®å€æ•°, 1500ã¾ã§ï¼‰ã§æŽ¨è«–ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™...\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 512 ã§æŽ¨è«–ä¸­ ---\n",
      "\n",
      "image 1/82 D:\\prog_py\\yolo12_detect\\detect_images\\024_01_b.jpg: 512x288 55 pods, 105.1ms\n",
      "image 2/82 D:\\prog_py\\yolo12_detect\\detect_images\\024_02_b.jpg: 512x224 59 pods, 63.3ms\n",
      "image 3/82 D:\\prog_py\\yolo12_detect\\detect_images\\024_02_f.jpg: 512x288 32 pods, 17.7ms\n",
      "image 4/82 D:\\prog_py\\yolo12_detect\\detect_images\\024_03_b.jpg: 512x320 46 pods, 72.8ms\n",
      "image 5/82 D:\\prog_py\\yolo12_detect\\detect_images\\024_03_f.jpg: 512x256 22 pods, 68.0ms\n",
      "image 6/82 D:\\prog_py\\yolo12_detect\\detect_images\\024_05_b.jpg: 512x256 38 pods, 16.1ms\n",
      "image 7/82 D:\\prog_py\\yolo12_detect\\detect_images\\024_05_f.jpg: 512x256 35 pods, 14.7ms\n",
      "image 8/82 D:\\prog_py\\yolo12_detect\\detect_images\\043_01_b.jpg: 512x352 31 pods, 122.7ms\n",
      "image 9/82 D:\\prog_py\\yolo12_detect\\detect_images\\043_01_f.jpg: 512x416 29 pods, 93.0ms\n",
      "image 10/82 D:\\prog_py\\yolo12_detect\\detect_images\\043_05_b.jpg: 512x288 20 pods, 18.5ms\n",
      "image 11/82 D:\\prog_py\\yolo12_detect\\detect_images\\043_05_f.jpg: 512x288 34 pods, 14.7ms\n",
      "image 12/82 D:\\prog_py\\yolo12_detect\\detect_images\\045_04_b.jpg: 512x352 44 pods, 16.7ms\n",
      "image 13/82 D:\\prog_py\\yolo12_detect\\detect_images\\045_04_f.jpg: 512x320 18 pods, 15.7ms\n",
      "image 14/82 D:\\prog_py\\yolo12_detect\\detect_images\\045_06_b.jpg: 480x512 38 pods, 82.9ms\n",
      "image 15/82 D:\\prog_py\\yolo12_detect\\detect_images\\045_06_f.jpg: 448x512 32 pods, 77.9ms\n",
      "image 16/82 D:\\prog_py\\yolo12_detect\\detect_images\\052_01_f.jpg: 512x416 30 pods, 14.4ms\n",
      "image 17/82 D:\\prog_py\\yolo12_detect\\detect_images\\052_02_f.jpg: 512x288 32 pods, 14.7ms\n"
     ]
    }
   ],
   "source": [
    "# æŒ‡å®šã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç”»åƒæŽ¨è«–ã‚’å®Ÿè¡Œ\n",
    "print(\"æŽ¨è«–ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "source_path = r\"D:\\prog_py\\yolo12_detect\\detect_images\"\n",
    "if os.path.exists(source_path):\n",
    "    print(\"ðŸ” ç”»åƒã‚µã‚¤ã‚ºï¼ˆ32ã®å€æ•°, 1500ã¾ã§ï¼‰ã§æŽ¨è«–ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™...\")\n",
    "\n",
    "    # 32ã®å€æ•°ã§512ã‹ã‚‰1248ã¾ã§\n",
    "    grid_imgsz = list(range(512, 1248, 32))\n",
    "\n",
    "    for imgsz in grid_imgsz:\n",
    "        print(f\"\\n--- ç”»åƒã‚µã‚¤ã‚º: {imgsz} ã§æŽ¨è«–ä¸­ ---\")\n",
    "        # å®Ÿé¨“åã‚’ç”»åƒã‚µã‚¤ã‚ºã”ã¨ã«åˆ†ã‘ã¦ä¿å­˜ã™ã‚‹ã“ã¨ã§å„ã‚µã‚¤ã‚ºã”ã¨ã«çµæžœãŒè¦‹ã‚„ã™ãã€ã™ã¹ã¦ã®ç”»åƒã‚µã‚¤ã‚ºã§æŽ¨è«–ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
    "        results = model.predict(\n",
    "            source=source_path,           # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€: detect_images\n",
    "            imgsz=imgsz,                  # ç”»åƒã‚µã‚¤ã‚ºã‚’å‹•çš„ã«å¤‰æ›´\n",
    "            save_txt=True,                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: True\n",
    "            line_width=1,                 # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®ç·šã®å¤ªã•: 1\n",
    "            save=True,                    # æŽ¨è«–çµæžœã®ç”»åƒã‚’ä¿å­˜\n",
    "            conf=0.25,                    # ä¿¡é ¼åº¦ã®ã—ãã„å€¤\n",
    "            device='0',                   # ãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPUãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯'0'ã«å¤‰æ›´ï¼‰\n",
    "            project='runs/detect',        # çµæžœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "            name=f'yolo_detection_{imgsz}',# ç”»åƒã‚µã‚¤ã‚ºã”ã¨ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ†ã‘\n",
    "            retina_masks=True,\n",
    "            agnostic_nms=True,\n",
    "        )\n",
    "\n",
    "print(\"âœ“ æŽ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(f\"çµæžœã¯ runs/detect/yolo_detection_ï¼œimgszï¼ž ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322475d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55c86a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” ç”»åƒã‚µã‚¤ã‚ºï¼ˆ32ã®å€æ•°, 1500ã¾ã§ï¼‰ã§æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™...\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 512 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "YOLO11l-seg summary (fused): 203 layers, 27,585,363 parameters, 0 gradients, 131.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 899.1118.3 MB/s, size: 190.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 12.9Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.1it/s 14.8s4.1s\n",
      "                   all         13        554      0.667      0.578      0.586      0.313       0.65      0.563       0.56      0.243\n",
      "Speed: 2.5ms preprocess, 46.9ms inference, 0.0ms loss, 18.7ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_512\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_512\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 544 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1093.8160.4 MB/s, size: 217.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.1it/s 13.7s2.2s\n",
      "                   all         13        554      0.696      0.565       0.61      0.341      0.712       0.56      0.597      0.271\n",
      "Speed: 1.5ms preprocess, 32.7ms inference, 0.0ms loss, 28.8ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_544\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_544\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 576 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1012.9156.5 MB/s, size: 188.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.1it/s 13.7s1.2s\n",
      "                   all         13        554       0.76      0.559      0.632      0.365      0.753      0.551      0.621      0.296\n",
      "Speed: 1.9ms preprocess, 41.7ms inference, 0.0ms loss, 15.4ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_576\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_576\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 608 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 953.7193.0 MB/s, size: 174.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.2it/s 13.1s9.3s\n",
      "                   all         13        554      0.692       0.63      0.644      0.378      0.692      0.623      0.635      0.302\n",
      "Speed: 2.3ms preprocess, 36.0ms inference, 0.0ms loss, 22.0ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_608\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_608\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 640 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1048.2158.2 MB/s, size: 180.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.2it/s 12.8s7.0s\n",
      "                   all         13        554      0.718      0.603      0.633      0.381      0.671      0.634      0.638      0.313\n",
      "Speed: 3.4ms preprocess, 55.3ms inference, 0.0ms loss, 30.8ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_640\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_640\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 672 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1019.1121.8 MB/s, size: 176.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.2it/s 11.2s4.8s\n",
      "                   all         13        554      0.704      0.626      0.643      0.387       0.71      0.632      0.641      0.323\n",
      "Speed: 2.0ms preprocess, 69.9ms inference, 0.0ms loss, 18.0ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_672\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_672\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 704 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1018.9130.2 MB/s, size: 207.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.2it/s 10.4s4.3s\n",
      "                   all         13        554      0.697      0.635      0.665      0.404      0.684      0.625       0.66      0.336\n",
      "Speed: 3.0ms preprocess, 53.7ms inference, 0.0ms loss, 19.8ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_704\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_704\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 736 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1090.0104.6 MB/s, size: 196.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.2it/s 10.3s5.1s\n",
      "                   all         13        554      0.684      0.641      0.662      0.414      0.687      0.643      0.657      0.351\n",
      "Speed: 2.7ms preprocess, 86.8ms inference, 0.0ms loss, 14.6ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_736\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_736\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 768 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1047.4144.9 MB/s, size: 205.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.2it/s 8.5s11.5s\n",
      "                   all         13        554       0.72      0.625      0.672      0.427      0.688      0.652      0.675      0.367\n",
      "Speed: 2.6ms preprocess, 58.7ms inference, 0.0ms loss, 32.6ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_768\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_768\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 800 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 887.477.0 MB/s, size: 180.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.2it/s 8.1s11.0s\n",
      "                   all         13        554      0.741      0.637      0.685       0.44      0.749      0.645      0.699      0.384\n",
      "Speed: 3.3ms preprocess, 63.4ms inference, 0.0ms loss, 22.2ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_800\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_800\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 832 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 511.972.4 MB/s, size: 193.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 12.3Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 7.7s10.9s\n",
      "                   all         13        554      0.714      0.659      0.687      0.442      0.711      0.654      0.693      0.384\n",
      "Speed: 3.8ms preprocess, 55.5ms inference, 0.0ms loss, 22.9ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_832\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_832\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 864 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 532.2137.4 MB/s, size: 187.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 7.6s10.3s\n",
      "                   all         13        554      0.722      0.626      0.681      0.454      0.734      0.637      0.696      0.394\n",
      "Speed: 3.9ms preprocess, 60.4ms inference, 0.0ms loss, 21.6ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_864\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_864\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 896 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1065.4215.0 MB/s, size: 201.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 6.9s9.5s\n",
      "                   all         13        554       0.75       0.63      0.695      0.468      0.729      0.657      0.709       0.41\n",
      "Speed: 3.6ms preprocess, 69.1ms inference, 0.0ms loss, 29.4ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_896\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_896\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 928 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1087.9223.2 MB/s, size: 194.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 6.6s10.0s\n",
      "                   all         13        554      0.743      0.659      0.701      0.476      0.745      0.662      0.712      0.421\n",
      "Speed: 3.9ms preprocess, 67.1ms inference, 0.0ms loss, 22.3ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_928\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_928\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 960 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 836.3215.1 MB/s, size: 171.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 6.2s10.5s\n",
      "                   all         13        554      0.705      0.644      0.693      0.481      0.707      0.646      0.702      0.425\n",
      "Speed: 4.4ms preprocess, 82.9ms inference, 0.0ms loss, 21.4ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_960\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_960\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 992 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1092.5275.4 MB/s, size: 194.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 6.0s9.4s\n",
      "                   all         13        554      0.751      0.659      0.712      0.493      0.752      0.662      0.719      0.434\n",
      "Speed: 4.7ms preprocess, 72.5ms inference, 0.0ms loss, 18.3ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_992\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_992\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 1024 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1165.084.4 MB/s, size: 195.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 6.0s9.5s\n",
      "                   all         13        554      0.747      0.679      0.718      0.502      0.754      0.686      0.734      0.442\n",
      "Speed: 4.6ms preprocess, 70.7ms inference, 0.0ms loss, 22.6ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1024\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1024\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 1056 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 742.6191.9 MB/s, size: 185.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 13.0Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 6.1s9.8s\n",
      "                   all         13        554      0.765      0.666      0.717      0.509      0.772      0.671      0.729      0.455\n",
      "Speed: 5.4ms preprocess, 74.7ms inference, 0.0ms loss, 27.6ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1056\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1056\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 1088 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1001.9114.2 MB/s, size: 174.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 6.4s11.2s\n",
      "                   all         13        554       0.76      0.641      0.726      0.507      0.758      0.639      0.724      0.451\n",
      "Speed: 6.6ms preprocess, 101.6ms inference, 0.0ms loss, 34.5ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1088\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1088\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 1120 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 948.4210.3 MB/s, size: 176.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 12.4Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.3it/s 6.5s11.8s\n",
      "                   all         13        554      0.754      0.641      0.719       0.51      0.758      0.644      0.731      0.458\n",
      "Speed: 6.4ms preprocess, 106.6ms inference, 0.0ms loss, 42.6ms postprocess per image\n",
      "Saving D:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1120\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\prog_py\\yolo12_detect\\runs\\val\\val_imgs_1120\u001b[0m\n",
      "\n",
      "--- ç”»åƒã‚µã‚¤ã‚º: 1152 ã§æ¤œè¨¼ä¸­ ---\n",
      "Ultralytics 8.3.223  Python-3.12.6 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 872.5141.0 MB/s, size: 166.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning \\\\?\\D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\labels\\val.cache... 13 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 0% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0/2  1.1s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imgsz \u001b[38;5;129;01min\u001b[39;00m grid_imgsz:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- ç”»åƒã‚µã‚¤ã‚º: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ã§æ¤œè¨¼ä¸­ ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m         validation_results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_yaml_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns/val\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_imgs_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimgsz\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:635\u001b[0m, in \u001b[0;36mModel.val\u001b[1;34m(self, validator, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[0;32m    634\u001b[0m validator \u001b[38;5;241m=\u001b[39m (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidator\u001b[39m\u001b[38;5;124m\"\u001b[39m))(args\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 635\u001b[0m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m validator\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\validator.py:215\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[1;34m(self, trainer, model)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 215\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[38;5;241m2\u001b[39m]:\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:663\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 663\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:139\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:157\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:180\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 180\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    181\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:266\u001b[0m, in \u001b[0;36mSegment.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    263\u001b[0m bs \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# batch size\u001b[39;00m\n\u001b[0;32m    265\u001b[0m mc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv4[i](x[i])\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl)], \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# mask coefficients\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mDetect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, mc, p\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:125\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m--> 125\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;28;01melse\u001b[39;00m (y, x)\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:166\u001b[0m, in \u001b[0;36mDetect._inference\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    164\u001b[0m x_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([xi\u001b[38;5;241m.\u001b[39mview(shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m x], \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m shape:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmake_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m shape\n\u001b[0;32m    169\u001b[0m box, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m x_cat\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\utils\\tal.py:376\u001b[0m, in \u001b[0;36mmake_anchors\u001b[1;34m(feats, strides, grid_cell_offset)\u001b[0m\n\u001b[0;32m    374\u001b[0m     sy, sx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(sy, sx, indexing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_11 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmeshgrid(sy, sx)\n\u001b[0;32m    375\u001b[0m     anchor_points\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mstack((sx, sy), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m--> 376\u001b[0m     stride_tensor\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(anchor_points), torch\u001b[38;5;241m.\u001b[39mcat(stride_tensor)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = YOLO(r\"D:\\prog_py\\yolo12_detect\\runs\\train\\train10\\weights\\best.pt\")\n",
    "# dataset.yamlã®ãƒ‘ã‚¹\n",
    "dataset_yaml_path = r\"D:\\Data\\pods\\2025_yolo_pod\\labels\\YOLODataset\\dataset.yaml\"\n",
    "\n",
    "# ç”»åƒã‚µã‚¤ã‚ºï¼ˆimgszï¼‰ã¯max stride 32ã®å€æ•°ã§ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ\n",
    "if os.path.exists(dataset_yaml_path):\n",
    "    print(\"ðŸ” ç”»åƒã‚µã‚¤ã‚ºï¼ˆ32ã®å€æ•°, 1500ã¾ã§ï¼‰ã§æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™...\")\n",
    "\n",
    "    # 32ã®å€æ•°ã§500ã‹ã‚‰1504ã¾ã§\n",
    "    grid_imgsz = list(range(512, 1248, 32))\n",
    "\n",
    "    for imgsz in grid_imgsz:\n",
    "        print(f\"\\n--- ç”»åƒã‚µã‚¤ã‚º: {imgsz} ã§æ¤œè¨¼ä¸­ ---\")\n",
    "        validation_results = model.val(\n",
    "            data=dataset_yaml_path,\n",
    "            imgsz=imgsz,\n",
    "            plots=True,\n",
    "            batch=8,\n",
    "            save_json=True,\n",
    "            device='0',\n",
    "            project='runs/val',\n",
    "            name=f'val_imgs_{imgsz}',\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a33570",
   "metadata": {},
   "source": [
    "##ã€€å‡ºåŠ›å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c58791",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load a model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mprog_py\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mYolo_trainer\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain9\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbest.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load a custom trained model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Export the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m              imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m)\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:83\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRTDETR\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m_get_name():  \u001b[38;5;66;03m# if RTDETR head\u001b[39;00m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:153\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:297\u001b[0m, in \u001b[0;36mModel._load\u001b[1;34m(self, weights, task)\u001b[0m\n\u001b[0;32m    294\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(weights)\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtask\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1517\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_checkpoint\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;124;03m    Load a single model weights.\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;124;03m        ckpt (dict): Model checkpoint dictionary.\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1517\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[0;32m   1518\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m   1519\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1464\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight, safe_only)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 ckpt \u001b[38;5;241m=\u001b[39m torch_load(f, pickle_module\u001b[38;5;241m=\u001b[39msafe_pickle)\n\u001b[0;32m   1463\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1464\u001b[0m             ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[0;32m   1467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\utils\\patches.py:120\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "\n",
    "model = YOLO(r\"D:\\prog_py\\Yolo_trainer\\runs\\train\\train9\\weights\\best.pt\")  # load a custom trained model\n",
    "\n",
    "# Export the model\n",
    "model.export(format=\"onnx\",\n",
    "             imgsz=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c0ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1204bd8",
   "metadata": {},
   "source": [
    "## auto label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d4539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/69 d:\\prog_py\\yolo12_detect\\Dataset\\insta360\\train\\024_01_b.jpg: 800x448 87 pods, 84.3ms\n",
      "image 2/69 d:\\prog_py\\yolo12_detect\\Dataset\\insta360\\train\\024_02_b.jpg: 800x352 97 pods, 76.5ms\n",
      "image 3/69 d:\\prog_py\\yolo12_detect\\Dataset\\insta360\\train\\024_02_f.jpg: 800x448 88 pods, 48.3ms\n",
      "image 4/69 d:\\prog_py\\yolo12_detect\\Dataset\\insta360\\train\\024_03_b.jpg: 800x480 92 pods, 73.7ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_annotate\n\u001b[1;32m----> 3\u001b[0m \u001b[43mauto_annotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataset\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43minsta360\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdet_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mprog_py\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43myolo12_detect\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain10\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbest.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43msam_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msam2.1_l.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\data\\annotator.py:60\u001b[0m, in \u001b[0;36mauto_annotate\u001b[1;34m(data, det_model, sam_model, device, conf, iou, imgsz, max_det, classes, output_dir)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_ids \u001b[38;5;241m:=\u001b[39m result\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mtolist():  \u001b[38;5;66;03m# Extract class IDs from detection results\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy  \u001b[38;5;66;03m# Boxes object for bbox outputs\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     sam_results \u001b[38;5;241m=\u001b[39m \u001b[43msam_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     segments \u001b[38;5;241m=\u001b[39m sam_results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmasks\u001b[38;5;241m.\u001b[39mxyn\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(output_dir)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mPath(result\u001b[38;5;241m.\u001b[39mpath)\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\models\\sam\\model.py:137\u001b[0m, in \u001b[0;36mSAM.__call__\u001b[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, bboxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    Perform segmentation prediction on the given image or video source.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        >>> print(f\"Detected {len(results[0].masks)} masks\")\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\models\\sam\\model.py:111\u001b[0m, in \u001b[0;36mSAM.predict\u001b[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    110\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(bboxes\u001b[38;5;241m=\u001b[39mbboxes, points\u001b[38;5;241m=\u001b[39mpoints, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:557\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:230\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:344\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\models\\sam\\predict.py:528\u001b[0m, in \u001b[0;36mPredictor.postprocess\u001b[1;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[0;32m    526\u001b[0m         pred_bboxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([pred_bboxes, pred_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m], \u001b[38;5;28mcls\u001b[39m[:, \u001b[38;5;28;01mNone\u001b[39;00m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[idx]\n\u001b[0;32m    527\u001b[0m         masks \u001b[38;5;241m=\u001b[39m masks[idx]\n\u001b[1;32m--> 528\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mResults\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_bboxes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# Reset segment-all mode.\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\prog_py\\yolo12_detect\\.venv\\Lib\\site-packages\\ultralytics\\engine\\results.py:240\u001b[0m, in \u001b[0;36mResults.__init__\u001b[1;34m(self, orig_img, path, names, boxes, masks, probs, keypoints, obb, speed)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mResults\u001b[39;00m(SimpleClass, DataExportMixin):\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    A class for storing and manipulating inference results.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m        >>>     result.plot()  # Plot detection results\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    242\u001b[0m         orig_img: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m    243\u001b[0m         path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    244\u001b[0m         names: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m    245\u001b[0m         boxes: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m         masks: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    247\u001b[0m         probs: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    248\u001b[0m         keypoints: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    249\u001b[0m         obb: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    250\u001b[0m         speed: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    251\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m        Initialize the Results class for storing and manipulating inference results.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m            13: Left Knee, 14: Right Knee, 15: Left Ankle, 16: Right Ankle\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_img \u001b[38;5;241m=\u001b[39m orig_img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics.data.annotator import auto_annotate\n",
    "\n",
    "auto_annotate(data=r\"Dataset\\insta360\\train\",\n",
    "              det_model=r\"D:\\prog_py\\yolo12_detect\\runs\\train\\train10\\weights\\best.pt\",\n",
    "              imgsz=800,\n",
    "              sam_model=\"sam2.1_l.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

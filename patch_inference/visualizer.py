"""
GIF Animation visualizer for patch-based detection
"""

import cv2
import numpy as np
import os
from pathlib import Path
from tqdm import tqdm
import imageio
from ultralytics import YOLO
from patched_yolo_infer import MakeCropsDetectThem, CombineDetections, visualize_results


# ========== ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š ==========
FRAME_INITIAL = 15       # åˆæœŸç”»åƒ
FRAME_FIRST_FADE = 10    # æœ€åˆã®ãƒ‘ãƒƒãƒãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³
FRAME_SLIDE = 8          # ãƒ‘ãƒƒãƒã‚¹ãƒ©ã‚¤ãƒ‰
FRAME_DETECT = 5         # æ¤œå‡ºä¸­
FRAME_FADE_IN = 8        # æ¤œå‡ºçµæœãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³
FRAME_PAUSE = 2          # æ¤œå‡ºå¾Œã®åœæ­¢
FRAME_NMS_BEFORE = 15    # NMSå‰
FRAME_TRANSITION = 12    # é·ç§»
FRAME_FINAL = 60         # æœ€çµ‚çµæœ

ALPHA_PAST = 0.15        # éå»ã®ãƒ‘ãƒƒãƒ
ALPHA_CURRENT = 0.75     # ç¾åœ¨ã®ãƒ‘ãƒƒãƒ


def draw_detections(img, detections_list, scale_factor_x=1.0, scale_factor_y=1.0, box_thickness=1):
    """
    æ¤œå‡ºçµæœï¼ˆãƒœãƒƒã‚¯ã‚¹+ãƒã‚¹ã‚¯ï¼‰ã‚’æç”»
    scale_factor_x: Xåº§æ¨™ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ï¼ˆworking_img â†’ å…ƒã‚µã‚¤ã‚ºï¼‰
    scale_factor_y: Yåº§æ¨™ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ï¼ˆworking_img â†’ å…ƒã‚µã‚¤ã‚ºï¼‰
    box_thickness: ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®ç·šå¹…
    """
    result = img.copy()
    for boxes, masks in detections_list:
        for i, box in enumerate(boxes):
            # åº§æ¨™ã‚’ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ï¼ˆXè»¸ã¨Yè»¸ã§ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ã‚’ä½¿ç”¨ï¼‰
            x1, y1, x2, y2 = box
            x1 = int(x1 * scale_factor_x)
            y1 = int(y1 * scale_factor_y)
            x2 = int(x2 * scale_factor_x)
            y2 = int(y2 * scale_factor_y)
            
            cv2.rectangle(result, (x1, y1), (x2, y2), (0, 255, 0), max(1, box_thickness))
            
            if i < len(masks):
                mask = masks[i]
                if mask.shape != result.shape[:2]:
                    mask = cv2.resize(mask, (result.shape[1], result.shape[0]),
                                    interpolation=cv2.INTER_NEAREST)
                colored_mask = np.zeros_like(result)
                colored_mask[mask > 0] = [0, 200, 0]
                result = cv2.addWeighted(result, 1, colored_mask, 0.35, 0)
    return result


def get_patch_regions(crops):
    """ã™ã¹ã¦ã®ãƒ‘ãƒƒãƒã®åº§æ¨™ã‚’å–å¾—ï¼ˆãƒªã‚µã‚¤ã‚ºå¾Œã®ç”»åƒãƒ™ãƒ¼ã‚¹ï¼‰"""
    patch_regions = []
    for crop in crops:
        h, w = crop.source_image_resized.shape[:2]
        x_min = max(0, int(crop.x_start))
        y_min = max(0, int(crop.y_start))
        crop_h, crop_w = crop.crop.shape[:2]
        x_max = min(w, int(crop.x_start + crop_w))
        y_max = min(h, int(crop.y_start + crop_h))
        patch_regions.append((x_min, y_min, x_max, y_max))
    return patch_regions


def create_overlap_map(patch_regions, img_shape):
    """ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãƒãƒƒãƒ—ã‚’ä½œæˆ"""
    h, w = img_shape[:2]
    overlap_map = np.zeros((h, w), dtype=np.uint8)
    for x_min, y_min, x_max, y_max in patch_regions:
        overlap_map[y_min:y_max, x_min:x_max] += 1
    return overlap_map


def draw_single_patch(img, region, overlap_map, alpha, show_overlap=True):
    """
    å˜ä¸€ã®ãƒ‘ãƒƒãƒã‚’æç”»ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—éƒ¨åˆ†ã¯æ¿ƒãï¼‰
    æ³¨ï¼šæ¸¡ã•ã‚ŒãŸç”»åƒã‚’å¤‰æ›´ã›ãšã€æ–°ã—ã„ç”»åƒã‚’è¿”ã—ã¾ã™
    """
    h, w = img.shape[:2]
    x_min, y_min, x_max, y_max = region
    
    # åº§æ¨™ã‚’ç”»åƒã‚µã‚¤ã‚ºå†…ã«ã‚¯ãƒªãƒƒãƒ—
    x_min = max(0, min(x_min, w))
    y_min = max(0, min(y_min, h))
    x_max = max(0, min(x_max, w))
    y_max = max(0, min(y_max, h))
    
    # ç©ºã®ãƒ‘ãƒƒãƒã¯ç„¡è¦–
    if x_max <= x_min or y_max <= y_min:
        return img
    
    result = img
    
    if show_overlap:
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãƒãƒƒãƒ—ã«åŸºã¥ã„ã¦æç”»
        patch_area = overlap_map[y_min:y_max, x_min:x_max]
        
        # é€šå¸¸éƒ¨åˆ†ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãªã—ï¼‰- è–„ã„é’è‰²
        normal_mask = (patch_area == 1)
        if np.any(normal_mask):
            overlay = np.zeros_like(img)
            patch_img = overlay[y_min:y_max, x_min:x_max]
            # ã‚µã‚¤ã‚ºãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            if patch_img.shape[:2] == normal_mask.shape[:2]:
                patch_img[normal_mask] = [255, 200, 150]  # è–„ã„é’è‰²
                result = cv2.addWeighted(result, 1.0, overlay, alpha * 0.6, 0)
        
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—éƒ¨åˆ†ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰- æ¿ƒã„é’è‰²
        overlap_mask = (patch_area > 1)
        if np.any(overlap_mask):
            overlay = np.zeros_like(img)
            patch_img = overlay[y_min:y_max, x_min:x_max]
            # ã‚µã‚¤ã‚ºãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            if patch_img.shape[:2] == overlap_mask.shape[:2]:
                patch_img[overlap_mask] = [255, 150, 100]  # æ¿ƒã„é’è‰²
                result = cv2.addWeighted(result, 1.0, overlay, alpha * 1.2, 0)
    else:
        # é€šå¸¸æç”» - è–„ã„é’è‰²
        overlay = np.zeros_like(img)
        cv2.rectangle(overlay, (x_min, y_min), (x_max, y_max), (255, 180, 120), -1)
        result = cv2.addWeighted(result, 1.0, overlay, alpha, 0)
    
    # å¢ƒç•Œç·š - è–„ã„é’è‰²
    cv2.rectangle(result, (x_min, y_min), (x_max, y_max), (255, 200, 150), 2)
    
    return result


def draw_multiple_patches(img, patch_regions, overlap_map, highlight_indices):
    """
    è¤‡æ•°ã®ãƒ‘ãƒƒãƒã‚’æç”»
    """
    result = img.copy()
    
    if not highlight_indices:
        return result
    
    for idx, alpha in highlight_indices.items():
        if idx < len(patch_regions):
            result = draw_single_patch(result, patch_regions[idx], overlap_map, alpha, show_overlap=True)
    
    return result


def create_detection_gif(
    image_path,
    model_path,
    output_dir,
    shape_x=400,
    shape_y=400,
    overlap_x=30,
    overlap_y=30,
    conf_threshold=0.5,
    imgsz=640,
    nms_threshold=0.3,
    fps=30,
    verbose=True,
    box_thickness=1,
    show_boxes=False,
    show_class=False,
    show_confidences=False,
    fill_mask=True,
    alpha=0.7,
    font_scale=2.0,
    random_object_colors=True,
    final_show_boxes=None
):
    """
    æ¤œå‡ºéç¨‹ã®GIFã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    
    Args:
        image_path: ç”»åƒãƒ‘ã‚¹
        model_path: YOLOãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        shape_x: ãƒ‘ãƒƒãƒã®å¹…
        shape_y: ãƒ‘ãƒƒãƒã®é«˜ã•
        overlap_x: Xè»¸ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ— (%)
        overlap_y: Yè»¸ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ— (%)
        conf_threshold: ä¿¡é ¼åº¦é–¾å€¤
        imgsz: YOLOå…¥åŠ›ã‚µã‚¤ã‚º
        nms_threshold: NMSé–¾å€¤
        fps: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆ
        verbose: è©³ç´°å‡ºåŠ›
        box_thickness: GIFå†…ã§æç”»ã™ã‚‹ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®ç·šå¹…
        show_boxes: æœ€çµ‚çµæœã«ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤ºã™ã‚‹ã‹
        show_class: æœ€çµ‚çµæœã«ã‚¯ãƒ©ã‚¹åã‚’è¡¨ç¤ºã™ã‚‹ã‹
        show_confidences: æœ€çµ‚çµæœã«ä¿¡é ¼åº¦ã‚’è¡¨ç¤ºã™ã‚‹ã‹
        fill_mask: æœ€çµ‚çµæœã§ãƒã‚¹ã‚¯ã‚’å¡—ã‚Šã¤ã¶ã™ã‹
        alpha: æœ€çµ‚çµæœãƒã‚¹ã‚¯ã®é€éåº¦
        font_scale: æœ€çµ‚çµæœãƒ©ãƒ™ãƒ«ã®æ–‡å­—ã‚µã‚¤ã‚º
        random_object_colors: æœ€çµ‚çµæœã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¯ã«ãƒ©ãƒ³ãƒ€ãƒ è‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        final_show_boxes: æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ ã§ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤ºã™ã‚‹ã‹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•åˆ¤å®šï¼‰
    
    Returns:
        ä¿å­˜ã•ã‚ŒãŸGIFã®ãƒ‘ã‚¹
    """
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é€£ç•ªã§ä½œæˆ
    base_dir = output_dir
    i = 1
    while True:
        output_dir = f"{base_dir}{i}" if i > 1 else base_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
            break
        i += 1
    
    if verbose:
        print(f"ğŸ¬ é«˜å“è³ªGIFä½œæˆä¸­: {Path(image_path).name}")
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model = YOLO(model_path)
    
    # ç”»åƒèª­ã¿è¾¼ã¿
    test_img = cv2.imread(image_path)
    if test_img is None:
        raise ValueError(f"ç”»åƒãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“: {image_path}")
    
    # ãƒ‘ãƒƒãƒãƒ™ãƒ¼ã‚¹æ¨è«–ï¼ˆãƒãƒƒãƒå‡¦ç†ã§é€šå¸¸æ¤œå‡ºã¨åŒã˜çµæœã‚’ä¿è¨¼ï¼‰
    element_crops = MakeCropsDetectThem(
        image=test_img,
        model=model,
        segment=True,
        shape_x=shape_x,
        shape_y=shape_y,
        overlap_x=overlap_x,
        overlap_y=overlap_y,
        conf=conf_threshold,
        imgsz=imgsz,
        show_crops=False,
        memory_optimize=False,
        batch_inference=True,  # ãƒãƒƒãƒå‡¦ç†ã§é€šå¸¸æ¤œå‡ºã¨ä¸€è‡´ã•ã›ã‚‹
        show_processing_status=False,
        resize_initial_size=True,
    )
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆï¼ˆRGBå½¢å¼ï¼‰
    frames = []
    
    # ãƒªã‚µã‚¤ã‚ºã•ã‚ŒãŸç”»åƒã‚’ä½¿ç”¨ï¼ˆãƒ‘ãƒƒãƒåº§æ¨™ã®å–å¾—ç”¨ï¼‰
    working_img = element_crops.crops[0].source_image_resized
    
    # ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ã‚’è¨ˆç®—ï¼ˆworking_img â†’ test_imgï¼‰
    # Xåº§æ¨™ã¨Yåº§æ¨™ã§ãã‚Œãã‚Œç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ã‚’ä½¿ç”¨
    scale_factor_x = test_img.shape[1] / working_img.shape[1]
    scale_factor_y = test_img.shape[0] / working_img.shape[0]

    gif_box_thickness = max(1, int(box_thickness))

    # æ¤œå‡ºçµæœã®åº§æ¨™ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°
    if getattr(element_crops, "resize_initial_size", True):
        detection_scale_x = 1.0
        detection_scale_y = 1.0
    else:
        detection_scale_x = scale_factor_x
        detection_scale_y = scale_factor_y
    
    # ãƒ‘ãƒƒãƒæƒ…å ±ã‚’å–å¾—ï¼ˆworking_imgã‚µã‚¤ã‚ºï¼‰
    patch_regions_working = get_patch_regions(element_crops.crops)
    
    # ãƒ‘ãƒƒãƒé ˜åŸŸã‚’å…ƒã‚µã‚¤ã‚ºã«ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ï¼ˆå¹…ã¨é«˜ã•ã§ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°ã‚’ä½¿ç”¨ï¼‰
    patch_regions = []
    for x_min, y_min, x_max, y_max in patch_regions_working:
        scaled_x_min = int(x_min * scale_factor_x)
        scaled_y_min = int(y_min * scale_factor_y)
        scaled_x_max = int(x_max * scale_factor_x)
        scaled_y_max = int(y_max * scale_factor_y)
        
        # ç”»åƒã‚µã‚¤ã‚ºå†…ã«ã‚¯ãƒªãƒƒãƒ—
        scaled_x_min = max(0, min(scaled_x_min, test_img.shape[1]))
        scaled_y_min = max(0, min(scaled_y_min, test_img.shape[0]))
        scaled_x_max = max(0, min(scaled_x_max, test_img.shape[1]))
        scaled_y_max = max(0, min(scaled_y_max, test_img.shape[0]))
        
        patch_regions.append((scaled_x_min, scaled_y_min, scaled_x_max, scaled_y_max))
    
    overlap_map = create_overlap_map(patch_regions, test_img.shape)
    num_patches = len(patch_regions)
    
    if verbose:
        print(f"  ãƒ‘ãƒƒãƒæ•°: {num_patches}å€‹")
        print(f"  å…ƒç”»åƒã‚µã‚¤ã‚º: {test_img.shape[1]}x{test_img.shape[0]}")
        print(f"  ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°: X={scale_factor_x:.3f}, Y={scale_factor_y:.3f}")
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—1: å…ƒç”»åƒ ==========
    if verbose:
        print("  ã‚¹ãƒ†ãƒƒãƒ—1: å…ƒç”»åƒè¡¨ç¤º")
    base_rgb = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)
    frames.extend([base_rgb.copy() for _ in range(FRAME_INITIAL)])
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—2: å„ãƒ‘ãƒƒãƒã‚’ã‚¹ãƒ©ã‚¤ãƒ‰ã—ãªãŒã‚‰æ¤œå‡º ==========
    if verbose:
        print("  ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ãƒƒãƒã‚¹ã‚­ãƒ£ãƒ³ã¨æ¤œå‡ºï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰è¡¨ç¤ºï¼‰")
    
    # æ¤œå‡ºçµæœã‚’ä¿å­˜ï¼ˆç´¯ç©æç”»ã—ãªã„ï¼‰
    all_detections = []  # [(boxes, masks), ...]
    
    # å‡¦ç†æ¸ˆã¿ãƒ‘ãƒƒãƒã‚’è¿½è·¡
    processed_patches = []
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§ãƒ‘ãƒƒãƒå‡¦ç†
    pbar = tqdm(enumerate(element_crops.crops), total=num_patches, 
                desc="  ãƒ‘ãƒƒãƒå‡¦ç†", unit="patch", ncols=100, disable=not verbose)
    
    for idx, crop in pbar:
        if verbose:
            pbar.set_postfix({"ãƒ‘ãƒƒãƒ": f"{idx+1}/{num_patches}"})
        
        # å‰ã®ãƒ‘ãƒƒãƒã‹ã‚‰ç¾åœ¨ã®ãƒ‘ãƒƒãƒã¸ã‚¹ãƒ©ã‚¤ãƒ‰
        if idx > 0:
            # ã‚¹ãƒ©ã‚¤ãƒ‰å‹•ä½œï¼ˆå‰ã®ãƒ‘ãƒƒãƒã‚’ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆã€æ¬¡ã®ãƒ‘ãƒƒãƒã‚’ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ï¼‰
            for step in range(FRAME_SLIDE):
                t = (step + 1) / FRAME_SLIDE
                
                # å‰ã®ãƒ‘ãƒƒãƒã®é€æ˜åº¦ã‚’ä¸‹ã’ã‚‹
                prev_alpha = ALPHA_CURRENT * (1 - t)
                # ç¾åœ¨ã®ãƒ‘ãƒƒãƒã®é€æ˜åº¦ã‚’ä¸Šã’ã‚‹
                curr_alpha = ALPHA_CURRENT * t
                
                highlight_dict = {}
                # ãã‚Œä»¥å‰ã®ãƒ‘ãƒƒãƒ
                for past_idx in processed_patches[:-1]:
                    highlight_dict[past_idx] = ALPHA_PAST
                # å‰ã®ãƒ‘ãƒƒãƒ
                if prev_alpha > 0.05:
                    highlight_dict[idx - 1] = prev_alpha
                # ç¾åœ¨ã®ãƒ‘ãƒƒãƒ
                if curr_alpha > 0.05:
                    highlight_dict[idx] = curr_alpha
                
                # æ¤œå‡ºçµæœã‚’æç”»ï¼ˆåº§æ¨™ã‚’ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ï¼‰
                display_img = draw_detections(
                    test_img,
                    all_detections,
                    detection_scale_x,
                    detection_scale_y,
                    gif_box_thickness
                )
                
                highlight_img = draw_multiple_patches(
                    display_img, patch_regions, overlap_map,
                    highlight_dict
                )
                frames.append(cv2.cvtColor(highlight_img, cv2.COLOR_BGR2RGB))
        else:
            # æœ€åˆã®ãƒ‘ãƒƒãƒï¼ˆãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ï¼‰
            for step in range(FRAME_FIRST_FADE):
                patch_alpha = 0.05 + (0.65 * (step + 1) / FRAME_FIRST_FADE)
                
                highlight_dict = {idx: patch_alpha}
                
                highlight_img = draw_multiple_patches(
                    test_img, patch_regions, overlap_map,
                    highlight_dict
                )
                frames.append(cv2.cvtColor(highlight_img, cv2.COLOR_BGR2RGB))
        
        # ãƒ‘ãƒƒãƒæ¤œå‡ºä¸­ï¼ˆç¾åœ¨ã®ãƒ‘ãƒƒãƒã‚’å¼·èª¿ï¼‰
        for _ in range(FRAME_DETECT):
            highlight_dict = {}
            for past_idx in processed_patches:
                highlight_dict[past_idx] = ALPHA_PAST
            highlight_dict[idx] = ALPHA_CURRENT  # æ¤œå‡ºä¸­ã¯æ˜ã‚‹ã
            
            display_img = draw_detections(
                test_img,
                all_detections,
                detection_scale_x,
                detection_scale_y,
                gif_box_thickness
            )
            
            highlight_img = draw_multiple_patches(
                display_img, patch_regions, overlap_map,
                highlight_dict
            )
            frames.append(cv2.cvtColor(highlight_img, cv2.COLOR_BGR2RGB))
        
        # ã“ã®ãƒ‘ãƒƒãƒã‚’å‡¦ç†æ¸ˆã¿ãƒªã‚¹ãƒˆã«è¿½åŠ 
        processed_patches.append(idx)
        
        # ãƒ‘ãƒƒãƒã‚’ã‚†ã£ãã‚Šè–„ãã™ã‚‹ï¼ˆãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆï¼‰
        for step in range(FRAME_SLIDE):
            patch_alpha = ALPHA_CURRENT - ((ALPHA_CURRENT - ALPHA_PAST) * (step + 1) / FRAME_SLIDE)
            
            highlight_dict = {}
            for past_idx in processed_patches[:-1]:  # å‰ã®ãƒ‘ãƒƒãƒ
                highlight_dict[past_idx] = ALPHA_PAST
            highlight_dict[idx] = patch_alpha  # ç¾åœ¨ã®ãƒ‘ãƒƒãƒã‚’ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆ
            
            display_img = draw_detections(
                test_img,
                all_detections,
                detection_scale_x,
                detection_scale_y,
                gif_box_thickness
            )
            
            highlight_img = draw_multiple_patches(
                display_img, patch_regions, overlap_map,
                highlight_dict
            )
            frames.append(cv2.cvtColor(highlight_img, cv2.COLOR_BGR2RGB))
        
        # å°‘ã—åœæ­¢ï¼ˆæ¤œå‡ºå‰ï¼‰
        for _ in range(FRAME_PAUSE):
            highlight_dict = {}
            for past_idx in processed_patches:
                highlight_dict[past_idx] = ALPHA_PAST
            
            display_img = draw_detections(
                test_img,
                all_detections,
                detection_scale_x,
                detection_scale_y,
                gif_box_thickness
            )
            
            highlight_img = draw_multiple_patches(
                display_img, patch_regions, overlap_map,
                highlight_dict
            )
            frames.append(cv2.cvtColor(highlight_img, cv2.COLOR_BGR2RGB))
        
        # æ¤œå‡ºçµæœã‚’ç´¯ç©ç”»åƒã«è¿½åŠ 
        det_boxes = []
        det_masks = []
        if len(crop.detected_xyxy_real) > 0:
            for i, box in enumerate(crop.detected_xyxy_real):
                det_boxes.append(box)
                if len(crop.detected_masks_real) > 0 and i < len(crop.detected_masks_real):
                    det_masks.append(crop.detected_masks_real[i])
        
        if det_boxes:
            all_detections.append((det_boxes, det_masks))
        
        # æ¤œå‡ºçµæœã‚’ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³
        if det_boxes:
            for step in range(FRAME_FADE_IN):
                det_alpha = (step + 1) / FRAME_FADE_IN
                
                # æ—¢å­˜ã®æ¤œå‡ºçµæœ
                display_img_base = draw_detections(
                    test_img,
                    all_detections[:-1],
                    detection_scale_x,
                    detection_scale_y,
                    gif_box_thickness
                )
                
                # æ–°ã—ã„æ¤œå‡ºçµæœã‚’ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³
                new_detections_img = draw_detections(
                    test_img,
                    [all_detections[-1]],
                    detection_scale_x,
                    detection_scale_y,
                    gif_box_thickness
                )
                
                blended = cv2.addWeighted(display_img_base, 1 - det_alpha, new_detections_img, det_alpha, 0)
                
                highlight_dict = {}
                for past_idx in processed_patches:
                    highlight_dict[past_idx] = ALPHA_PAST
                
                highlight_img = draw_multiple_patches(
                    blended, patch_regions, overlap_map,
                    highlight_dict
                )
                frames.append(cv2.cvtColor(highlight_img, cv2.COLOR_BGR2RGB))
        
        # æ¤œå‡ºçµæœãŒè¿½åŠ ã•ã‚ŒãŸå¾Œã®åœæ­¢
        for _ in range(FRAME_PAUSE):
            highlight_dict = {}
            for past_idx in processed_patches:
                highlight_dict[past_idx] = ALPHA_PAST
            
            display_img = draw_detections(
                test_img,
                all_detections,
                detection_scale_x,
                detection_scale_y,
                gif_box_thickness
            )
            
            highlight_img = draw_multiple_patches(
                display_img, patch_regions, overlap_map,
                highlight_dict
            )
            frames.append(cv2.cvtColor(highlight_img, cv2.COLOR_BGR2RGB))
    
    pbar.close()
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—3: NMSå‰ã®å…¨æ¤œå‡ºçµæœ ==========
    if verbose:
        print("  ã‚¹ãƒ†ãƒƒãƒ—3: NMSå‰ã®å…¨æ¤œå‡ºçµæœ")
    
    # ã™ã¹ã¦ã®æ¤œå‡ºçµæœã‚’æç”»
    nms_before_img = draw_detections(
        test_img,
        all_detections,
        detection_scale_x,
        detection_scale_y,
        gif_box_thickness
    )
    
    all_patches_dict = {i: 0.12 for i in range(num_patches)}
    final_with_grid = draw_multiple_patches(nms_before_img, patch_regions, overlap_map, 
                                            all_patches_dict)
    frames.extend([cv2.cvtColor(final_with_grid, cv2.COLOR_BGR2RGB) for _ in range(FRAME_NMS_BEFORE)])
    
    # NMSå‰ã®æ¤œå‡ºã‚’ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆï¼ˆãƒ‘ãƒƒãƒã‚°ãƒªãƒƒãƒ‰ã¯æ®‹ã™ï¼‰
    clean_base_temp = test_img.copy()
    grid_only = draw_multiple_patches(clean_base_temp, patch_regions, overlap_map,
                                     all_patches_dict)
    grid_only_rgb = cv2.cvtColor(grid_only, cv2.COLOR_BGR2RGB)
    
    for step in range(8):  # 8ãƒ•ãƒ¬ãƒ¼ãƒ ã§ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆ
        fade_alpha = 1.0 - ((step + 1) / 8.0)  # 1.0 â†’ 0.0
        
        # NMSå‰ã®æ¤œå‡ºã‚’ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆ
        fade_out = cv2.addWeighted(
            grid_only_rgb, 1 - fade_alpha,
            cv2.cvtColor(final_with_grid, cv2.COLOR_BGR2RGB), fade_alpha, 0
        )
        frames.append(fade_out)
    
    # ãƒ‘ãƒƒãƒã‚°ãƒªãƒƒãƒ‰ã®ã¿ã‚’æ•°ãƒ•ãƒ¬ãƒ¼ãƒ è¡¨ç¤ºï¼ˆå®Œå…¨ã«ã‚¯ãƒªãƒ¼ãƒ³ï¼‰
    frames.extend([grid_only_rgb.copy() for _ in range(8)])
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—4: NMSé©ç”¨ã¨æœ€çµ‚çµæœ ==========
    if verbose:
        print("  ã‚¹ãƒ†ãƒƒãƒ—4: NMSé©ç”¨ã¨æœ€çµ‚çµæœ")
    
    result = CombineDetections(
        element_crops, 
        nms_threshold=nms_threshold,
        class_agnostic_nms=True,  # ã‚¯ãƒ©ã‚¹é–“ã§ã‚‚NMSã‚’é©ç”¨
    )
    
    if verbose:
        print(f"  NMSå‰ã®æ¤œå‡ºæ•°: {len([b for det in all_detections for b in det[0]])}å€‹")
        print(f"  NMSå¾Œã®æ¤œå‡ºæ•°: {len(result.filtered_boxes)}å€‹")
    
    # å®Œå…¨ã«æ–°ã—ã„ã‚¯ãƒªãƒ¼ãƒ³ãªç”»åƒã‚’ç”¨æ„ï¼ˆå…ƒç”»åƒã‹ã‚‰ç›´æ¥ã‚³ãƒ”ãƒ¼ï¼‰
    clean_img_for_final = test_img.copy()
    
    # ã‚¯ãƒªãƒ¼ãƒ³ãªç”»åƒã§NMSå¾Œã®çµæœã‚’å¯è¦–åŒ–ï¼ˆbboxãªã—ã€ãƒã‚¹ã‚¯ã®ã¿ï¼‰
    final_show_boxes_flag = (
        final_show_boxes
        if final_show_boxes is not None
        else (show_boxes and not fill_mask)
    )

    final_img = visualize_results(
        img=clean_img_for_final,
        boxes=result.filtered_boxes,
        classes_ids=result.filtered_classes_id,
        confidences=result.filtered_confidences,
        classes_names=result.filtered_classes_names,
        masks=result.filtered_masks,
        segment=True,
        show_boxes=final_show_boxes_flag,
        show_class=show_class,
        fill_mask=fill_mask,
        alpha=alpha,
        thickness=max(1, int(box_thickness)),
        font_scale=font_scale,
        show_confidences=show_confidences,
        return_image_array=True,
        random_object_colors=random_object_colors,
    )
    
    final_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
    
    # ãƒ‘ãƒƒãƒã‚°ãƒªãƒƒãƒ‰ã‚’ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆã—ãªãŒã‚‰æœ€çµ‚çµæœã¸é·ç§»
    for step in range(FRAME_TRANSITION - 1):  # æœ€å¾Œã®1ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é™¤ã
        transition_alpha = (step + 1) / FRAME_TRANSITION
        
        # ãƒ‘ãƒƒãƒã‚°ãƒªãƒƒãƒ‰ã®é€æ˜åº¦ã‚’ä¸‹ã’ã‚‹
        fading_patches_dict = {i: 0.12 * (1 - transition_alpha) for i in range(num_patches)}
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãªç”»åƒã«ãƒ‘ãƒƒãƒã‚°ãƒªãƒƒãƒ‰ã‚’é‡ã­ã‚‹
        clean_base = test_img.copy()
        grid_on_clean = draw_multiple_patches(clean_base, patch_regions, overlap_map,
                                             fading_patches_dict)
        
        # æœ€çµ‚çµæœã¸ãƒ–ãƒ¬ãƒ³ãƒ‰
        transition = cv2.addWeighted(
            cv2.cvtColor(grid_on_clean, cv2.COLOR_BGR2RGB), 1 - transition_alpha,
            final_rgb, transition_alpha, 0
        )
        frames.append(transition)
    
    # æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ ã¯å®Œå…¨ã«final_rgbã®ã¿ï¼ˆé·ç§»ã®æœ€å¾Œï¼‰
    frames.append(final_rgb.copy())
    
    # æœ€çµ‚çµæœã‚’é•·ã‚ã«è¡¨ç¤º
    frames.extend([final_rgb.copy() for _ in range(FRAME_FINAL)])
    
    # ========== GIFä¿å­˜ ==========
    if verbose:
        print("  ã‚¹ãƒ†ãƒƒãƒ—5: GIFä¿å­˜ä¸­...")
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ã¯æ—¢ã«å…ƒã‚µã‚¤ã‚ºã§ä½œæˆã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãƒªã‚µã‚¤ã‚ºä¸è¦
    image_basename = Path(image_path).stem
    gif_path = os.path.join(output_dir, f"{image_basename}_detection.gif")
    
    if verbose:
        print("  GIFæ›¸ãè¾¼ã¿ä¸­...")
    with tqdm(total=100, desc="  GIFä¿å­˜", unit="%", ncols=100, disable=not verbose) as pbar:
        imageio.mimsave(
            gif_path,
            frames,
            fps=fps,
            loop=0
        )
        pbar.n = 100
        pbar.refresh()
    
    if verbose:
        print(f"\nâœ… GIFä½œæˆå®Œäº†!")
        print(f"   ğŸ“ ä¿å­˜å…ˆ: {gif_path}")
        print(f"   ğŸ“Š ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(frames)}æš")
        print(f"   â±ï¸  å†ç”Ÿæ™‚é–“: ç´„{len(frames)/fps:.1f}ç§’")
        print(f"   ğŸ¯ æ¤œå‡ºæ•°: {len(result.filtered_confidences)}å€‹")
        print(f"   ğŸ“ ç”»åƒã‚µã‚¤ã‚º: {test_img.shape[1]}x{test_img.shape[0]}")
    
    return gif_path

